{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # Install dependency # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Python 3.8.3 to 3.9.0 to 3.10 ( 3.10 is experimental with ray)\n",
    "# # # unQuote to install dependency\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install numpy\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for Nvidia gpu / cpu version  # I didn't try with ROCm for AMD gpu (pytorch.org) if you want to look at it\n",
    "# update nvidia driver\n",
    "# download and install cuda from : https://developer.nvidia.com/cuda-11-6-1-download-archive\n",
    "# install cudnn :  https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\n",
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # for cpu only | the model is fairly small for cartpole for example, so it should be able to run well on cpu only\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "\n",
    "# # # unQuote to install dependency\n",
    "# # # Gym env for game simulation\n",
    "# # install for windows Microsoft C++ Build Tools 14+ (require for Box2D lib):\n",
    "# # https://visualstudio.microsoft.com/downloads/\n",
    "# !pip3 install gym[all]\n",
    "# !pip3 install gym[atari]\n",
    "# !pip install gym[accept-rom-license]\n",
    "# # or\n",
    "# conda install -c conda-forge gym-all\n",
    "# if you can't install Box2D, try :\n",
    "# for windows: conda install -c anaconda swig\n",
    "# or else try the docker.\n",
    "\n",
    "# # # Ray install\n",
    "# pip install -U \"ray[default]\"\n",
    "\n",
    "# #### OR #####\n",
    "# # replace  \" !pip3 \"  by \" !pip \" depending on the OS\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# # # for cloud compute : \n",
    "# https://docs.ray.io/en/latest/cluster/vms/getting-started.html#vm-cluster-quick-start\n",
    "# https://docs.ray.io/en/latest/train/train.html\n",
    "# - create ray kubernetes cluster at your cloud provider. (with or without gpu)\n",
    "# - add kubernetes cluster address. (can also specify the number of gpu/cpu in\n",
    "#   use) to ray.init() of self_play in self_play.py for remote cluster compute.\n",
    "# - wrap model muzero_model.train() in self_play.py into a ray.init() and add\n",
    "# kubernetes cluster address (can also specify the number of gpu/cpu in use).\n",
    "# - then use function or cli as usual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from self_play import Show_all_gym_env,Show_specific_gym_env_structure\n",
    "# print all the available gym env\n",
    "# Show_all_gym_env()\n",
    "        \n",
    "# # # show spaces of specific env\n",
    "# Show_specific_gym_env_structure(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from monte_carlo_tree_search import *\n",
    "from game import *\n",
    "from replay_buffer import *\n",
    "from self_play import *\n",
    "from muzero_model import *\n",
    "\n",
    "\n",
    "# # # unquote to print complet tensor\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python muzero_cli train report config/experiment_421_config.json\n",
    "#or\n",
    "# # # set game environment from gym library\n",
    "# # # render_mode should be set to None if you don't want rgb observation\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None) \n",
    "# # # if you want want rgb observation set render_mode to \"rgb_array\" , \"human\" or the render_mode rgb of our env\n",
    "# # # so for example if you want to use vision_model, you will have to change it\n",
    "# env = gym.make(\"ALE/Asterix-v5\",render_mode='human')\n",
    "\n",
    "# # # the random seed are set to 0 for reproducibility purpose\n",
    "# # #  https://pytorch.org/docs/stable/notes/randomness.html\n",
    "seed = 0\n",
    "np.random.seed(seed) # set the random seed of numpy\n",
    "torch.manual_seed(seed) # set the random seed of pytorch\n",
    "\n",
    "\n",
    "# # # init/set muzero model for training and inference\n",
    "muzero = Muzero(model_structure = 'mlp_model', # 'vision_model' : will use rgb as observation , 'mlp_model' : will use game state as observation\n",
    "                observation_space_dimensions = env.observation_space, # dimension of the observation \n",
    "                action_space_dimensions = env.action_space, # dimension of the action allow (gym box/discrete)\n",
    "                state_space_dimensions= 61, # support size / encoding space (keep state smaller than hidden layer and use odd number)\n",
    "                hidden_layer_dimensions = 126, # number of weight in the recursive layer of the mlp\n",
    "                number_of_hidden_layer= 4, # number of recusion layer of hidden layer of the mlp\n",
    "                k_hypothetical_steps= 10, # number of future step you want to be simulate during train (they are mainly support loss)\n",
    "                learning_rate= 0.01,# learning rate of the optimizer\n",
    "                optimizer = \"adam\", # optimizer \"adam\" or \"sgd\"\n",
    "                lr_scheduler = \"cosineannealinglr\",# learning rate scheduler\n",
    "                loss_type = \"general\", # muzero loss can be \"general\" or \"game\"\n",
    "                num_of_epoch = 1000, # number of step use by lr_scheduler\n",
    "                device=\"cpu\", # hardware on which to compute : \"cpu\" , \"cuda\" (it will auto scale on multi gpu or cpu for training, not inference)\n",
    "                type_format = torch.float32, # choice the dtype of the model. look at [https://pytorch.org/docs/1.8.1/amp.html#ops-that-can-autocast-to-float16]\n",
    "                load = False, # function to load a save model\n",
    "                use_amp = False, # use mix precision (will get more accuracy than single single precision for smaller dtype like torch.float16. amp do not support torch.float64. will turn amp to True by fault for torch.float16)\n",
    "                bin_method = \"uniform_bin\", # \"linear_bin\" , \"uniform_bin\" : will have a regular incrementation of action or uniform sampling(pick randomly) from the bound\n",
    "                bin_decomposition_number = 10, # number of action to sample from low/high bound of a gym discret box\n",
    "                priority_scale = 0.5,\n",
    "                rescale_value_loss = 1) \n",
    "\n",
    "demonstration_buffer = DemonstrationBuffer()\n",
    "# demonstration_buffer.load_back_up_buffer(\"config/427_hbuffer.pickle\")\n",
    "\n",
    "# # # init/set the game storage(stor each game) and dataset(create dataset) generate during training\n",
    "replay_buffer = ReplayBuffer(window_size = 500, # number of game store in the buffer\n",
    "                             batch_size = 128, # batch size is the number of observe game during train\n",
    "                             num_unroll = muzero.k_hypothetical_steps, # number of mouve/play store inside the batched game\n",
    "                             td_steps = 50, # number of step the value is select and scale on \n",
    "                             game_sampling = \"priority\", # 'uniform' or \"priority\" (will game randomly or with a priority distribution)\n",
    "                             position_sampling = \"priority\",# 'uniform' or \"priority\" (will sample position in game randomly or with a priority distribution)\n",
    "                             reanalyze_stack = [ReanalyseBuffer(),\n",
    "                                                demonstration_buffer,\n",
    "                                                MostRecentBuffer(max_buffer_size = 10),\n",
    "                                                HighestRewardBuffer(max_buffer_size = 10)],\n",
    "                             reanalyse_fraction=0., # porcentage/100 of reanalyze vs new_game (set to 0 is equivalent to use muzero with online training only)\n",
    "                             reanalyse_fraction_mode = \"chance\" # \"chance\" or \"ratio\"\n",
    "                             ) \n",
    "\n",
    "# # # init/set the monte carlos tree search parameter\n",
    "mcts = Monte_carlo_tree_search(num_simulations = 0,# number of level (length of the tree )\n",
    "                               maxium_action_sample = 2,# number of node per level ( width of the tree )\n",
    "                               number_of_player = 1, # will cycle linearly to each player\n",
    "                               pb_c_base = 19652, \n",
    "                               pb_c_init = 1.25,\n",
    "                               discount = 0.997, \n",
    "                               root_dirichlet_alpha = 0.25, \n",
    "                               root_exploration_fraction = 0.25,\n",
    "                               custom_loop = None) # can create a custome cycle like : \"1>2>3>3\" ( 1->2->3->3->1->2->3->3->1...)\n",
    "\n",
    "# # # ini/set the Game class which embbed the gym game class function\n",
    "gameplay = Game(limit_of_game_play = 500, # maximum number of mouve , by default float(\"inf\")\n",
    "                gym_env = env, \n",
    "                discount = mcts.discount,\n",
    "                observation_dimension = muzero.observation_dimension, \n",
    "                action_dimension = muzero.action_dimension,\n",
    "                rgb_observation = muzero.is_RGB,\n",
    "                action_map = muzero.action_dictionnary,\n",
    "                priority_scale= muzero.priority_scale)\n",
    "\n",
    "\n",
    "print( f\"Dimension of the observation space : {muzero.observation_dimension} \\\n",
    "         Dimension of the action space : {muzero.action_dimension}\")\n",
    "\n",
    "\n",
    "# # # train model (if you choice vison model it will render the game)\n",
    "epoch_pr , loss ,reward,learning_config = learning_cycle(number_of_iteration = 1000, # number of epoch(step) in  muzero should be the |total amount of number_of_iteration x number_of_training_before_self_play|\n",
    "                                          number_of_self_play_before_training = 10, # number of game played record in the replay buffer before training\n",
    "                                          number_of_training_before_self_play = 1, # number of epoch cpmpute by the model before selplay\n",
    "                                          model_tag_number = 450, # tag number use to generate checkpoint\n",
    "                                          temperature_type = \"static_temperature\" , # \"static_temperature\" ,\"linear_decrease_temperature\" ,  \"extreme_temperature\" and \"reversal_tanh_temperature\"\n",
    "                                          verbose = True, # if you want to print the epoch|reward|loss during train\n",
    "                                          number_of_worker_selfplay = 0, # \"max\" will set the max amount of cpu core, 0 will make selflay run sequentially. Parallelize self-play on the number of worker\n",
    "                                          muzero_model = muzero,\n",
    "                                          gameplay = gameplay,\n",
    "                                          monte_carlo_tree_search = mcts,\n",
    "                                          replay_buffer = replay_buffer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_play import report, generate_config_file \n",
    "\n",
    "report( muzero, replay_buffer, epoch_pr, loss, reward, verbose = True)\n",
    "\n",
    "generate_config_file(env,seed,muzero,replay_buffer,mcts,gameplay,learning_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python muzero_cli play config/experiment_421_config.json\n",
    "#or\n",
    "from self_play import play_game_from_checkpoint \n",
    "import torch\n",
    "\n",
    "play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                          model_tag = 450,\n",
    "                          model_device = \"cpu\",\n",
    "                          model_type = torch.float32,\n",
    "                          mcts_pb_c_base=19652 , \n",
    "                          mcts_pb_c_init=1.25, \n",
    "                          mcts_discount= 0.997, \n",
    "                          mcts_root_dirichlet_alpha=0.25, \n",
    "                          mcts_root_exploration_fraction=0.25,\n",
    "                          mcts_with_or_without_dirichlet_noise = True,\n",
    "                          number_of_monte_carlo_tree_search_simulation = 0,\n",
    "                          maxium_action_sample = 2,# number of node per level ( width of the tree )\n",
    "                          number_of_player = 1, \n",
    "                          custom_loop = None,\n",
    "                          temperature = 0,\n",
    "                          game_iter = 2000,\n",
    "                          slow_mo_in_second = 0.0,\n",
    "                          render = True,\n",
    "                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python muzero_cli benchmark config/experiment_421_config.json\n",
    "#or\n",
    "# benchmark \n",
    "from self_play import play_game_from_checkpoint,benchmark\n",
    "import torch\n",
    "\n",
    "number_of_trial = 100\n",
    "cache_t,cache_r,cache_a,cache_p = [],[],[],[]\n",
    "for _ in range(number_of_trial):\n",
    "    tag , reward , action, policy = play_game_from_checkpoint(game_to_play = 'CartPole-v1',\n",
    "                                                                model_tag = 450,\n",
    "                                                                model_device = \"cpu\",\n",
    "                                                                model_type = torch.float32,    \n",
    "                                                                mcts_pb_c_base=19652 , \n",
    "                                                                mcts_pb_c_init=1.25, \n",
    "                                                                mcts_discount= 0.997, \n",
    "                                                                mcts_root_dirichlet_alpha=0.25, \n",
    "                                                                mcts_root_exploration_fraction=0.25,\n",
    "                                                                mcts_with_or_without_dirichlet_noise = True,\n",
    "                                                                number_of_monte_carlo_tree_search_simulation = 0, \n",
    "                                                                maxium_action_sample = 2,\n",
    "                                                                number_of_player = 1, \n",
    "                                                                custom_loop = None, \n",
    "                                                                gameplay_discount = 0.997,  \n",
    "                                                                temperature = 0,\n",
    "                                                                game_iter = 500,\n",
    "                                                                slow_mo_in_second = 0,\n",
    "                                                                render = False,\n",
    "                                                                verbose = False,\n",
    "                                                                benchmark = True) \n",
    "    #could do it in one list or even wrap the play_game with benchmark but it reduce clarity\n",
    "    cache_t.append(tag)\n",
    "    cache_r.append(reward)\n",
    "    cache_a.append(action)\n",
    "    cache_p.append(policy)\n",
    "\n",
    "\n",
    "benchmark(cache_t,\n",
    "          cache_r,\n",
    "          cache_a,\n",
    "          cache_p,\n",
    "          folder = \"report\",\n",
    "          verbose = True)\n",
    "\n",
    "#on cartpole the reward is fix to 1, so it follow the number of mouve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pentos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8b498c6143e427b0550c5476a07819fd54c33e7113c46248f934dfaabc2ba6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
